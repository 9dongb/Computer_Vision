### 1. MLP(Multi Layer Perceptron)
- 최소 하나 이상의 은닉 계층을 포함한 퍼셉트론
- Linear Layer(Fully Connected Layer)로 구성
- 선형 시스템 -> 비선형 시스템으로 변환하기 위해 **활성화함수(Activation Function)** 사용
- MLP기반 분류 모델은 Linear Layer, Actication function으로 구성

### 2. MLP 기반의 분류기 제작 순서 pytorch  코드 작성 순서
1. 모듈 import (torch torch.nn 등)
2. Data transform 정의 (torchvision.transforms)
3. Data load (torch 내장 데이터 활용 시, torch.utils.data)
4. 모델 설계 (MLP)
5. 모델 생성 손실함수 설정(torch.nn), 최적화 알고리즘 설정(torch.optim)
6. 모델 학습/테스트 코드 작성

### 3. Batch Size
- Small Batch Size : 과소적합(Underfitting)
- Optimized Batch Size : 이상적인(Ideal)
- High Batch Size : 과대적합(Overfitting)

### 4. ReLU (Activation Function)
- ReLU = max(0, x)
- 양수는 그대로, 음수는 0으로 반환하는 비선형 함수
- 선형함수 : 동차성과 가산성을 모두 만족하는 함수
- 동차성 : f(a*b) = a*f(b)
- 가산성 : f(a+b) = f(a)+f(b)

### 5. Softmax
- 모든 값을 0~1 사이로 Normalize하며, 모든 값들의 합이 1이 되게 만드는 함수

### 6. Log Softmax
- Softmax의 출력에 자연 로그(log)를 취한 것
- 숫자 안정성과 계산 효율성을 개선
### 6-1. Log Softmax 사용 시 이점 (F.log_softmax)
- 수치 안정성(Numerical Stability) : 직접적인 softmax 계산은 지수 함수 때문에 매우 큰 값으로 이어질 수 있지만, Log Sofrmax는 이러한 문제를 완화
- 효율적인 역전파(Backpropagation) : 로그 공간에서는 곱셈이 덧셈으로 바뀌어 계산이 더 간단하고 효율적이 됨
- 교차 엔트로피 손실과의 결합 : 교차 엔트로피 손실을 계산할 때, log softmax의 출력은 직접 사용될 수 있어 별도의 로그 계산 단계를 줄일 수 있음

### 7. 최적화 관련 대표 파라미터
- Batch Size : 학습 과정에 한번에 사용되는 데이터 샘플의 수 (Mini-batch Size)
- Epoch : 전체 데이터 셋을 몇 번 동안 학습에 활용할 것인지 결정 (Mini-batch 1회 학습은 Iteration 으로 표기)
- Learning Rate : 모델 가중치의 업데이트 크기를 정하는 파라미터로, 학습 속도를 조절
- Weight Decay : 가중치의 크기를 제한하여 모델이 학습 데이터의 노이즈에 덜 민감하게 만드는 Parameter로 과적합 방지를 위해 사용 (일반화 성능 개선)
- Momentum : 이전 업데이트를 고려하여 현재 업데이트의 방향을 조절하는 파라미터, Local minima에서 벗어나도록 도움
